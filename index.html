<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" slick-uniqueid="3">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="stylesheet" href="./Homepage_files/sty/jemdoc.css" type="text/css">
<style type="text/css">
</style>
<title>Xiaolong Yang Homepage</title>
<link rel="shortcut icon" href="./Homepage_files/fig/profile_image.jpg" >
</head>


<body>

<div id="layout-content" style="margin-top:25px">

<table>
<tbody>
  <tr>
    <td width="19%" valign="top" height="173">
      <img height="225" id="photo" style="padding: 0pt 30pt 0pt 20pt; float: left; display: inline;" src="./Homepage_files/fig/profile_image.jpg">
    </td>
    
    <td width="80%" valign="top" height="173">
      <b><font face="Times New Roman" size="6">Xiaolong Yang</font><font size="6" face="楷体_GB2312"> （杨小龙）</font><font face="Times New Roman" size="6"></font></b>
      <br><br>      
      <p>
         <table>
         <tbody>
            <tr>
              <td> <img width="20" style="float: left; display: inline;" src="./Homepage_files/fig/platform.png" alt=""> </td>
              <td> Data Platform, Tencent</td>
           </tr>
         </tbody>
         </table>
         <table>
         <tbody>
            <tr>
              <td> <img width="20" style="float: left; display: inline;" src="./Homepage_files/fig/amss.png" alt=""> </td>
              <td> <a href="http://mmrc.amss.cas.cn/" target="_blank">Key Laboratory of Mathematics-Mechanization</a>(KLMM) of <a href="http://www.amss.ac.cn/" target="_blank">Academy of Mathematics and Systems Science of the Chinese Academy of Sciences</a>(AMSS, CAS)</td>
            </tr>
         </tbody>
         </table>
         <table>
         <tbody>
            <tr>
              <td> <img width="20" style="float: left; display: inline;" src="./Homepage_files/fig/ucas.png" alt=""> </td>
              <td> <a href="https://english.ucas.ac.cn/" target="_blank">University of the Chinese Academy of Sciences </a>(UCAS)</td>
            </tr>
         </tbody>
         </table>        
          <br>
          <table>
         <tbody>
            <tr>
              <td> <img width="20" style="float: left; display: inline;" src="./Homepage_files/fig/email.png" alt=""> </td>
              <td> Email: xiaolonyang@tencent.com  [or]  yangxiaolong17@mails.ucas.ac.cn</td>
            </tr>
         </tbody>
         </table>
         
      </p>
    </td>
  </tr>
</tbody>
</table>

<style>
  ul {
    list-style: none; /* 移除默认的列表样式 */
    counter-reset: li; /* 初始化计数器 */
  }
  ul li {
    margin: 0;
    padding: 0;
    text-indent: -0.87em;  /* 0.87在换行时能留出大概的间距 */
    padding-left: 0.87em;
  }
  ul li:before {
    counter-increment: li; /* 每次使用计数器，其值增加1 */
    content: counter(li) ". "; /* 在每个<li>元素前添加序号 */
    display: inline-block; /* 设置为内联块元素 */
    width: 15px; /* 设置序号固定宽度 */
    text-align: right; /* 序号右对齐 */
  }
</style>
<h2>Works</h2>
<table id="tbWorks" border="0" width="100%">
  <tbody>
    <tr>
      <td> Dec.2023 - Now &nbsp; </td>    
      <td style="border-bottom:1px solid lightgray;"> 
           <br>
           <font size="3" face="楷体_GB2312" style="color: #4e72b8;"> 文生视频-双语视文CLIP</font> 
           <ul>
           <li><font size="2" face="宋体">实现基于中英文词表的扩充训练，CN图文检索超过当前最强中文模型，EN图文检索略微弱于OpenCLIP；</font></li>
           <li><font size="2" face="宋体">改造图文接口为视文对象，训练视文CLIP并注重中文本地特色化的语义对齐，实现高性能中文的视文检索。</font></li>
           </ul>    
      </td>
    </tr>
    <tr>
      <td> Aug.2023 - Dec.2023 &nbsp; </td>   
      <td style="border-bottom:1px solid lightgray;"> 
           <br>
           <font size="3" face="楷体_GB2312" style="color: #4e72b8;"> 业务大模型(金融)-预训练、SFT的完整链路</font> 
           <ul>
           <li><font size="2" face="宋体">基于英文的llama2-70B模型拓展中文和金融能力，探索扩充词表和训练方案的最优组合；合作方Few-shot测试对比：(with llama2原始模型)英文能力+0，中文能力+7.5，金融能力+8.1；(with GPT3.5模型)英文能力-1.7，中文能力+5.6，金融能力+11；</font></li>
           <li><font size="2" face="宋体">金融的指令微调训练，获得财报总结、行情异动分析等To B业务的可交互的定制化功能；</font></li>
           <li><font size="2" face="宋体">性能优越的预训练模型提供给保险业务大模型、通用SFT大模型研究等多个业务作为base model使用。</font></li>
           </ul>
      </td>
    </tr>
    <tr>
      <td> Apr.2023 - Aug.2023 &nbsp; </td>   
      <td style="border-bottom:1px solid lightgray;"> 
           <br>
           <font size="3" face="楷体_GB2312" style="color: #4e72b8;"> 通用大模型(混元助手)-强化学习：RM、PPO和DPO</font> 
           <ul>
           <li><font size="2" face="宋体">全面提升原有SFT模型(热、冷启动)在NLP基础任务、逻辑推理、多轮对话、领域应用等多个方面的薄弱不足项，版本发测多次领先；</font></li>
           <li><font size="2" face="宋体">提出引导式推理方案，调整ppo-ptx中主模型和奖励模型的交互策略，高效解决线上用户体验中的重点badcase问题；数学、物理计算题多次测试正确率提高约三倍，首次正确率0%->87%，强化模型的泛化推理能力得到显著提升；</font></li>
           <li><font size="2" face="宋体">176b RM的训练和迭代，最终奖励模型的验证精度(模型对同源数据的学习能力)为76.96% ，对比竞品llama2 RM： Safety 64.3%、Helpfulness 70.6%； 测试精度(模型对异源数据的泛化能力)为70.41% ，对比构造prompt使用GPT4自动打分44.0% 。</font></li>
           </ul>
      </td>
    </tr>
    <tr>
      <td> Feb.2023 - Apr.2023 &nbsp; </td>   
      <td style="border-bottom:1px solid lightgray;"> 
           <br>
           <font size="3" face="楷体_GB2312" style="color: #4e72b8;"> 业务大模型(广告)-营销话术的SFT和强化学习</font> 
           <ul>
           <li><font size="2" face="宋体">实现基于医美、口腔、眼科、植发等重点行业信息的AIGC封闭式问答机器人；现阶段已经在消费医疗赛道-口腔打造正向案例，其中头部客户对比人工设置的问答，AI生成问答沉淀的线索有效率提升44%(37.93%→54.55%)，成本降低21%(1110.0→918.4)；</font></li>
           <li><font size="2" face="宋体">基于客户喜好和转化需求，使用强化学习技术实现问答话术的个性化表达，兼具语言风格、交流能力、商品知识和营销导向等定制化的核心原子能力：拟人化程度高、态度亲切温和；理解商品相关行业属性知识图谱以及用户非专业表达的诉求；话术新鲜多样促进留存。</font></li>
           </ul>
      </td>
    </tr>
    <tr>
      <td> Jan.2023 - Feb.2023 &nbsp; </td>   
      <td style="border-bottom:1px solid lightgray;"> 
           <br>
           <font size="3" face="楷体_GB2312" style="color: #4e72b8;"> 文生图-SOTA的复现以及超越性能的改造</font> 
           <ul>
           <li><font size="2" face="宋体">根据业务需求，完成闭源工作DiT的复现；改造latent空间为像素级别进行训练，使用更大的参数量的transformer结构代替U-Net；</font></li>
           <li><font size="2" face="宋体">ImageNet 256x256数据集上实验结果超过当前SOTA(FID：2.22>2.27)。</font></li>
           </ul>
      </td>
    </tr> 
    <tr>
      <td > Oct.2022 - Feb.2023 &nbsp; </td>   
      <td style="border-bottom:1px solid lightgray;"> 
           <br>
           <font size="3" face="楷体_GB2312" style="color: #4e72b8;"> 人脸识别-工业应用的高难度case解决方案</font> 
           <ul>
           <li><font size="2" face="宋体">人脸检索逻辑方案在人脸检测/识别性能上均大幅超越竞争对手，递交给合作方唯一突破0.9的高精度技术方案；</font></li>
           <li><font size="2" face="宋体">实现了真人/漫画两种场景的集成一体化服务，无需针对漫画场景使用额外模型或切换服务底层调整参数；</font></li>
           <li><font size="2" face="宋体">重点人物准召优化，大规模'涉政'人物召回指标0.9836远远高于竞品0.0717，二级标签acc指标为0.87，远超业务方需求0.8；</font></li>
           <li><font size="2" face="宋体">高度模糊场景、真人/相片多人多脸混合场景、黑白旧照片锐化失真场景、戴口罩等遮挡场景、漫画夸张场景的定点优化。</font></li>
           </ul>
      </td>
    </tr>
    <tr>
      <td> Aug.2022 - Oct.2022 &nbsp; </td>   
      <td style="border-bottom:1px solid lightgray;">
           <br>
           <font size="3" face="楷体_GB2312" style="color: #4e72b8;"> 隐私保护计算-大数据加密的高精度概率回归模型</font> 
           <ul>
           <li><font size="2" face="宋体">开发联合概率密度与错误标签后验概率的高精度模型，针对隐私加密通信数据具有极高的检索/判断精度。对于1K/5K/1W数量的未知加密样本，模型平均精度从98+提升至99.9/99.7/99.6；</font></li>
           <li><font size="2" face="宋体">PSI抽样特征从27降至7个，在实现高精度加密通信的同时单次运行效率达万分之三秒左右；</font></li>
           <li><font size="2" face="宋体">参加iDASH 2022-Task 4，并在该赛道以99.39的超高精度超过蚂蚁集团(去年双赛道冠军，99.04)等所有竞争对手斩获模型精度第一，综合隐私通信效率后获得第二名。[<a href="./Awards/22iDASH.jpg" target="_blank">Prize</a>]</font></li>
           </ul>
      </td>
    </tr>
    <tr>
      <td> Jun.2022 - Aug.2022 &nbsp; </td>   
      <td style="border-bottom:1px solid lightgray;">
           <br>
           <font size="3" face="楷体_GB2312" style="color: #4e72b8;"> 广告特征工程-视文相关性和精排特征实验</font> 
           <ul>
           <li><font size="2" face="宋体">通用TVR算法开发，实验结果对比多数前沿算法有超过15%的性能提升，与当前大模型SOTA方法HunYuan_TVR性能相当；</font></li>
           <li><font size="2" face="宋体">广告业务场景TVR算法，R1结果表明潜在自动化审核处理效率预估能提高约10%，全行业测试集分类准确率87.3%；</font></li>
           <li><font size="2" face="宋体">关联广告特征数据与用户特征数据，实现广告特征精排上线流程；完成公众号文章/广告双emb特征相关性推送上线。</font></li>
           </ul>
      </td>
    </tr>
  </tbody>
</table> 



  
<h2>Biography</h2>
<p style="text-align:justify;">
  I am currently a researcher at Data Platform, Tencent. I received my PhD degree at <a href="http://www.amss.ac.cn/" target="_blank">Academy of Mathematics and Systems Science of the Chinese Academy of Sciences (AMSS, CAS)</a> and <a href="https://english.ucas.ac.cn/" target="_blank"> University of Chinese Academy of Sciences (UCAS) </a>. My supervisor is Prof.<a href="http://www.mmrc.iss.ac.cn/~xhjia/" target="_blank"> Xiaohong Jia </a>. Before that,   I received my Bachelor's degree of <a href="http://math.nwpu.edu.cn/en/home.htm" target="_blank"> Information and Computing Science</a> from <a href="https://en.nwpu.edu.cn/" target="_blank"> Northwestern Polytechnical University (NWPU)</a> in 2017. 
</p>
<p> 
  My current research spot is AIGC, including large models and multi-modality. My research interest also includes computer graphics and computer vision. My resume can be found [<a href="./MyCV.pdf" target="_blank"> here </a>].
</p>

<h2>Publications</h2>
     
  <table border="0" width="100%">
  <tbody> 

          <!-- LARNeXt TPAMI2023 -->
    <tr>
      <td>
        <div align="center">
          <img width="180" height="85" src="./LARNeXt(TPAMI2023)/Fig0_headpic.png" alt="">
        </div>
      </td>
      <td>
        <a href="https://ieeexplore.ieee.org/abstract/document/10143393" target="_blank">LARNeXt: End-to-End Lie Algebra Residual Network for Face Recognition</a>        
        <br>
        <strong>Xiaolong Yang</strong>, 
        Xiaohong Jia, 
        Dihong Gong,
        Dong-Ming Yan,
        Zhifeng Li,
        Wei Liu
        <br> 
        <font size="2.5"><i>IEEE Transactions on Pattern Analysis and Machine Intelligence(T-PAMI)</i>, Vol.45, No.10, pp.11961 - 11976, 2023.</font>
        <br>
        [<a href="LARNet.html" target="_blank">Project Page</a>]
        [<a href="./LARNeXt(TPAMI2023)/LARNeXt_End-to-End_Lie_Algebra_Residual_Network_for_Face_Recognition.pdf" target="_blank">PDF</a>]
        [<a href="https://github.com/paradocx/LARNet/" target="_blank">Code</a>]
        [<a href="./LARNet(ICML2021)/xiaolonyang_ICML21_Sup.pdf" target="_blank">Supplement</a>] 
        [<a href="./LARNet(ICML2021)/ICML21_LARNet.pdf" target="_blank">Slides</a>]
      </td>
    </tr>
    

        <!-- LARNet ICML2021 -->
    <tr>
      <td>
        <div align="center">
          <img width="180" height="85" src="./LARNet(ICML2021)/Fig0_headpic.png" alt="">
        </div>
      </td>
      <td>
        <a href="http://proceedings.mlr.press/v139/yang21d.html" target="_blank">LARNet: Lie Algebra Residual Network for Face Recognition</a>        
        <br>
        <strong>Xiaolong Yang</strong>, 
        Xiaohong Jia, 
        Dihong Gong,
        Dong-Ming Yan,
        Zhifeng Li,
        Wei Liu
        <br> 
        <i>In Proceedings of the 38th International Conference on Machine Learning (ICML2021)</i>
        <br>
        [<a href="LARNet.html" target="_blank">Project Page</a>]
        [<a href="./LARNet(ICML2021)/xiaolonyang_ICML2021_Main.pdf" target="_blank">PDF</a>]
        [<a href="https://github.com/paradocx/LARNet/" target="_blank">Code</a>]
        [<a href="./LARNet(ICML2021)/xiaolonyang_ICML21_Sup.pdf" target="_blank">Supplement</a>] 
        [<a href="./LARNet(ICML2021)/ICML21_LARNet.pdf" target="_blank">Slides</a>]
      </td>
    </tr>
    
    
     <!-- PoseEstimation JCST2021 -->
    <tr>
      <td>
        <div align="center">
           <img width="180" height="85" src="./6DPose(JCST2021)/res.png"  alt="">
        </div>
      </td>
      <td>
        <a href=" https://jcst.ict.ac.cn/EN/10.1007/s11390-021-1311-2" target="_blank">6D Object Pose Estimation in Cluttered Scenes from RGB Images</a>        
        <br>
        <strong>Xiaolong Yang</strong>, 
        Xiaohong Jia, 
        Yuan Liang,
        Lubin Fan
        <br> 
        <i>Journal of Computer Science and Technology (JCST)</i>, Vol.37, No.3, pp.719-730, 2022. 
        <br>
        [<a href="./6DPose(JCST2021)/JCST2021PDF.pdf" target="_blank">PDF</a>]
        [<a href="https://github.com/paradocx/LARNet/" target="_blank">Code</a>]
        [<a href="./6DPose(JCST2021)/Highlight.pdf" target="_blank">Slides</a>]
      </td>
    </tr>
    
    
         <!-- SimplePrimitiveRecognition CVM2020 -->
    <tr>
      <td>
        <div align="center">
          <img width="180" height="85" src="./SPR(CVM2020)/res.jpg" alt="">
        </div>
      </td>
      <td>
        <a href=" https://link.springer.com/article/10.1007/s41095-020-0192-6" target="_blank">Simple Primitive Recognition via Hierarchical Face Clustering</a>        
        <br>
        <strong>Xiaolong Yang</strong>, 
        Xiaohong Jia
        <br> 
        <i>Computational Visual Media (CVM)</i>, Vol.6, No.4, pp.431-443, 2020.
        <br>
        [<a href="./SPR(CVM2020)/YangSPR2020.pdf" target="_blank">PDF</a>]
        [<a href="https://github.com/paradocx/LARNet/" target="_blank">Code</a>]
        [<a href="./SPR(CVM2020)/CVMzl.pdf" target="_blank">Patent</a>]
      </td>
    </tr>
    
    
    <!-- 6DPose SIG2020 -->
    <tr>
      <td>
        <div align="center">
          <img width="180" height="85" src="./6DPose(SIGP2020)/res.png" alt="">
        </div>
      </td>
      <td>
        <a href="https://dl.acm.org/doi/10.1145/3388770.3407423" target="_blank">6D Pose Estimation with Two-stream Net</a>        
        <br>
        <strong>Xiaolong Yang</strong>, 
        Xiaohong Jia
        <br> 
        <i>ACM SIGGRAPH Posters</i>, No.40, pp.1-2, 2020.
        <br>
        [<a href="./6DPose(SIGP2020)/YangSIGP2020.pdf" target="_blank">PDF</a>]
        [<a href="https://github.com/paradocx/LARNet/" target="_blank">Code</a>]
        [<a href="https://dl.acm.org/doi/10.1145/3388770.3407423" target="_blank">Slides&Talk</a>]
        
      </td>
    </tr>
    
    
     <!-- FacePose TST2020 -->
    <tr>
      <td>
        <div align="center">
          <img width="180" height="85" src="./FacePose(TST2020)/res.png" alt="">
        </div>
      </td>
      <td>
        <a href="https://ieeexplore.ieee.org/abstract/document/9036145" target="_blank">Real-Time Facial Pose Estimation and Tracking by Coarse-to-Fine Iterative Optimization</a>        
        <br>
        <strong>Xiaolong Yang</strong>, 
        Xiaohong Jia,
        Mengke Yuan,
        Dong-Ming Yan
        <br> 
        <i>Tsinghua Science and Technology (TST)</i>, Vol.25, No.5, pp.690-700, 2020.
        <br>
        [<a href="./FacePose(TST2020)/YangTST2020.pdf" target="_blank">PDF</a>]
        [<a href="https://github.com/paradocx/LARNet/" target="_blank">Code</a>]
      </td>
    </tr>
    
    
         <!-- Diving MPT2019 -->
    <tr>
      <td>
        <div align="center">
          <img width="180" height="85" src="./Diving(MPT2019)/res.png" alt="">
        </div>
      </td>
      <td>
        <a href="https://www.cnki.com.cn/Article/CJFDTotal-SSJS201916003.htm" target="_blank">Physical Model Analysis and Body Shape Modification of Platform Diving</a> <font size="1.5" face="楷体_GB2312"> （跳台跳水的物理模型分析和体型修正）</font>       
        <br>
        <strong>Xiaolong Yang</strong>, 
        RongPing Shen,
        Ziyin Zhang
        <br> 
        <strong><font color="#FF0000">First Prize</font></strong> of the 15th China Post-Graduate Mathematical Contest in Modeling, recommended to <i>Journal of Mathematics in Practice and Theory</i>, Vol.49, No.16, pp.35-45, 2019.
        <br>
        [<a href="./Diving(MPT2019)/YangMPT2019.pdf" target="_blank">PDF</a>]
        [<a href="./Diving(MPT2019)/prize.pdf" target="_blank">Prize</a>]
      </td>
    </tr>
    
    
  </tbody>
</table>  


 <h2>Honorary Awards</h2>
<table id="tbAwards" border="0" width="100%">
  <tbody>
    <tr>
      <td> 2018&nbsp; </td>   
      <td> 
       <a href="./Awards/18MM.pdf" target="_blank"><strong> First Prize </strong></a>  of the 15th China Post-Graduate Mathematical Contest in Modeling [<font color="#FF0000" size="3" face="Times New Roman"> <i>Top 1%</i> </font>]   </td>
    </tr>
    <tr>
      <td> 2019&nbsp; </td>   
      <td> 
           <a href="./Awards/19PMS.pdf" target="_blank"><strong> Pacemaker to Merit Student </strong></a>  of CAS <font size="3" face="楷体_GB2312"> (中科院三好学生标兵) </font> [<font color="#FF0000" size="3" face="Times New Roman"> <i>Top 1%</i> </font>] </td>
    </tr>
    <tr>
      <td> 2020-2022 (3 years)  </td>   
      <td> Excellent Merit Student of CAS <font size="3" face="楷体_GB2312"> (中科院优秀三好学生)</font> [<a href="./Awards/20MS.pdf" target="_blank">2020</a>, <a href="./Awards/21MS.pdf" target="_blank">2021</a>,<a href="./Awards/22MS.pdf" target="_blank">2022</a>]</td>
     </tr>  
      <tr>
      <td> 2021&nbsp;</td>   
      <td>   
      <a href="./Awards/21NP.pdf" target="_blank"><strong> National Scholarship </strong></a>   <font size="3" face="楷体_GB2312"> (国家奖学金)</font>  [<font color="#FF0000" size="3" face="Times New Roman"> <i>Top 1%</i> </font>]
       </td>
    </tr>  
  </tbody>
</table> 


 
<h2>Research Experiences</h2>
<table id="tbExperiences" border="0" width="100%">
  <tbody>
    <tr>
      <td> Sep.2020 - Sep.2021 &nbsp; </td>   
      <td> Visting student (Rhino-Bird Program), Tencent Technologies Co., Ltd. (Shenzhen), directed by <a href="https://scholar.google.com/citations?user=VTrRNN4AAAAJ&hl=zh-en" target="_blank"> Dr. Zhifeng Li</td>
    </tr>
    <tr>
      <td> Nov.2019 - Mar.2020 &nbsp; </td>   
      <td> Intern student (Innovative Research Program), Alibaba Group (Beijing). </td>
    </tr>
    <tr>
      <td> Mar.2017 - Sep.2021 &nbsp; </td>   
      <td> Visiting student,  National Laboratory of Pattern Recognition(NLPR), Institute of Automation, Chinese Academy of Sciences(CASIA) (Beijing), directed by <a href="https://sites.google.com/site/yandongming/" target="_blank"> Prof. Dong-Ming Yan</td>
    </tr>  
  </tbody>
</table> 


 <h2>Academic Services</h2>
<table id="tbExperiences" border="0" width="100%">
  <tbody>
    <tr>
      <td> Conference Reviewer：  &nbsp; </td>   
      <td> NeurIPS, ICML, ICLR, CVPR, ICCV, ECCV, AAAI, CVM ... </td>
    </tr>
    <tr>
      <td> Journal Reviewer：  &nbsp; </td>   
      <td> T-PAMI, T-CSVT, T-NNLS, Neurocomputing, The Visual Computer ... </td>
    </tr>
  
  </tbody>
</table> 

</div>
</body>
</html>
